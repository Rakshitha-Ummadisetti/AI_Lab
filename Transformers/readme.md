This notebook is a complete implementation of Transformer architecture using Numpy. This experiment gives a detail walkthrough of the math behind.

This repo includes:
Setup & Hyperparameters  
2. Tokenization & Vocabulary  
3. Token Embeddings + Positional Encoding  
4. Core Building Blocks (Attention, FFN, LayerNorm)  
5. Encoder Stack (N layers)  
6. Decoder Stack (N layers)  
7. Full Transformer Model  
8. Loss Function & Adam Optimizer with Warmup  
9. Training Loop  
10. Attention Heatmap Visualizations  
11. Autoregressive Greedy Decoding  
12. Beam Search Decoding  

Paper link: https://arxiv.org/abs/1706.03762
